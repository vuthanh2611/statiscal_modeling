---
title: "Hospital dataset"
author: "Vu Duc Thanh"
output:
  html_document: default
  pdf_document: default
---

```{r message=FALSE}
library(expsmooth)
data(hospital)
plot(hospital[,705],type='l')

```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(forecast)
library(tseries)
library(lmtest)
library(Hmisc)


df <- hospital[,705]


train <- window(df, end  =c(year=2005,  month=12))
test  <- window(df, start=c(year=2006,month=1))

```
STL decomposition
```{r, echo=FALSE, fig.height=8, fig.width=10}
plot(stl(df, s.window="periodic"))
```

Box-Cox transformation with optimal $\lambda$:
```{r, echo=FALSE, fig.width=10, fig.height=8}
par(mfrow=c(2,1))
plot(df, ylab="Original series", xlab="", col="red")
grid()

LambdaOpt <- BoxCox.lambda(df)
plot(BoxCox(df, LambdaOpt), ylab="Transformed series", xlab="", col="red")
title(main=toString(round(LambdaOpt, 3)))
grid()
```

Transformation clearly stabilizes the variance, so it makes sense to use it. 
What if we round $\lambda$ to 0?
```{r, echo=FALSE, fig.height=4, fig.width=10}
plot(BoxCox(df, 0), ylab="Transformed series", xlab="", col="red")
title(main="0")
grid()
```
Looks the same. We'll use $\lambda=0$ from now on.

```{r, echo=FALSE}
LambdaOpt <- 0
```
## ARIMA
### Automatic model selection
Using auto.arima:
```{r, echo=FALSE}
fit.auto <- auto.arima(df, lambda=LambdaOpt)
fit.auto
```
ARIMA(2,0,0)(2,0,0)$_{12}$ is selected. Here are the residuals:
```{r, echo=FALSE, fig.height=4.5, fig.width=10}
res.auto <- residuals(fit.auto)
plot(res.auto)

```
```{r, echo=FALSE, fig.height=4.5, fig.width=10}
tsdisplay(res.auto)
```
```{r, echo=FALSE}
p <- rep(0, 1, frequency(df)*3)
for (i in 1:length(p)){
  p[i] <- Box.test(res.auto, lag=i, type = "Ljung-Box")$p.value
}
plot(p, xlab="Lag", ylab="p-values", ylim=c(0,1), main="Ljung-Box test")
abline(h = 0.05, lty = 2, col = "blue")
```
```{r, echo=FALSE, fig.height=5.5, fig.width=10}
par(mfrow=c(1,2))
qqnorm(res.auto)
qqline(res.auto, col="red")
hist(res.auto)
```

Hypothesis   | Test         | Result         | P-value
------------ | ------------ | -------------- | ------------------------------
Normality    | Shapiro-Wilk | rejected       | `r shapiro.test(res.auto)$p.value`
Unbiasedness | Wilcoxon     | not rejected   | `r wilcox.test(res.auto)$p.value`
Stationarity | KPSS         | not rejected   | `r kpss.test(res.auto)$p.value`

Fitting the selected model to the first $T-D$ points of the series to check the accuracy of the forecast on the last $D$ points:
```{r, echo=FALSE}
fitShort <- Arima(train, order=c(2,0,0), seasonal=c(2,0,0), lambda=LambdaOpt)
fc       <- forecast(fitShort, h=12)
accuracy(fc, test)
```

```{r, echo=FALSE, fig.height=5.5, fig.width=10}
plot(forecast(fitShort, h=12),xlab="Year")
lines(df, col="red")
```

### Manual model tuning
The series is stationary (p> `r kpss.test(BoxCox(df, LambdaOpt))$p.value`, KPSS test) 


Look at ACF and PACF of the obtained series:

```{r, echo=FALSE, fig.height=5.5, fig.width=12}
par(mfrow=c(1,2))
acf(BoxCox(df, LambdaOpt), lag.max=5*12, main="")
pacf(BoxCox(df, LambdaOpt), lag.max=5*12, main="")
```
Autocorrelation is significantly different from 0 for lags 1,2,3,14,15,16,17,18,28,30,36 
Since 12 is maximal significant seasonal lag, we could use $Q = 3 = 36/12$ as an initial approximation.
Maximal significant lag before 36 is 30, hence the starting value $q=30$.

Partial autocorrelation is significantly different from 0 for lags 1, 3,14. 
Following the same logic as above, we select initial values $P=1$, $p=3$.

Next we'll look for the best models with auto.arima using d=0, D=0, max.p=4, max.q=31, max.P=2, max.Q=4 (we added 1 to every initial approximation found above just in case), and the parameters of the previous model as starting points of the search. Also, max.order is set to max.p+max.q+max.P+max.Q.

```{r echo=F}
fit <- auto.arima(df, d=0, D=0, max.p=4, max.q=31, max.P=2, max.Q=4, max.order=29, 
                  start.p=2, start.q=1, start.P=2, start.Q=1, 
                  lambda=LambdaOpt, biasadj=T)#, stepwise=F  ,parallel=T)
fit
```



## Final model selection

Comparing the residuals of two ARIMAs:
```{r, echo=FALSE, fig.height=8, fig.width=8}
res      <- residuals(fit, type = "response")
res.auto <- residuals(fit.auto, type = "response")

plot(res, res.auto, xlim=c(min(res, res.auto), max(res, res.auto)), ylim=c(min(res, res.auto), max(res, res.auto)), 
     xlab = "Residuals of manually found model", ylab="Residuals of auto.arima model")
grid()
lines(c(min(res, res.auto), max(res, res.auto))*2, c(min(res, res.auto), max(res, res.auto))*2, col="red")
```
```{r, echo=F}
dm.test(res, res.auto)
```
The model now is exact the same with the auto model. Compare with ETS model

## ETS model
```{r, echo=FALSE}
fit.ets <- ets(df, model="AAA",lambda=LambdaOpt)
print(fit.ets)
```

Residuals:
```{r, echo=FALSE, fig.height=8, fig.width=10}
tsdisplay(residuals(fit.ets))
```

Ljung-Box test p-values for them:

```{r, echo=FALSE}
p <- rep(0, 1, frequency(df)*3)
for (i in 1:length(p)){
  p[i] <- Box.test(residuals(fit.ets), lag=i, type = "Ljung-Box")$p.value
}
plot(p, xlab="Lag", ylab="p-values", ylim=c(0,1), main="Ljung-Box test")
abline(h = 0.05, lty = 2, col = "blue")
```



```{r, echo=FALSE, fig.height=5.5, fig.width=10}
par(mfrow=c(1,2))
qqnorm(residuals(fit.ets))
qqline(residuals(fit.ets), col="red")
hist(residuals(fit.ets))
```

Hypothesis   | Test         | Result         | P-value
------------ | ------------ | -------------- | ------------------------------
Normality    | Shapiro-Wilk | rejected       | `r shapiro.test(residuals(fit.ets))$p.value`
Unbiasedness | Wilcoxon     | not rejected   | `r wilcox.test(residuals(fit.ets))$p.value`
Stationarity | KPSS         | not rejected   | `r kpss.test(residuals(fit.ets))$p.value`

```{r,warning=FALSE}
shapiro.test(residuals(fit.ets))$p.value
wilcox.test(residuals(fit.ets))$p.value
kpss.test(residuals(fit.ets))$p.value

```

Fitting the selected model to the first $T-D$ points of the series to check the accuracy of the forecast on the last $D$ points:
```{r, echo=FALSE}
fitShort <- ets(train, model="AAA", damped=F, lambda=LambdaOpt)
fc       <- forecast(fitShort, h=12)
accuracy(fc, test)
```
```{r, echo=FALSE, fig.height=5.5, fig.width=10}
plot(forecast(fitShort, h=12),  xlab="Year")
lines(df, col="red")
```

Comparing the residuals of the best ARIMA and the best ETS models:
```{r fig.width=8, fig.height=8, echo=FALSE}
res.ets <- residuals(fit.ets, type = "response")

plot(res.auto, res.ets, 
     xlab="Residuals, best ARIMA",
     ylab="Residuals, best ETS",
     xlim=c(min(c(res.auto, res.ets), na.rm=T), max(c(res.auto, res.ets), na.rm=T)),
     ylim=c(min(c(res.auto, res.ets), na.rm=T), max(c(res.auto, res.ets), na.rm=T)))
 lines(c(min(c(res.auto, res.ets), na.rm=T), max(c(res.auto, res.ets), na.rm=T)), c(min(c(res.auto, res.ets), na.rm=T), max(c(res.auto, res.ets), na.rm=T)), col="red")
```
```{r, echo=F}
dm.test(res.auto, res.ets)
dm.test(res.auto, res.ets, alternative = "greater")
```
Diebold-Mariano test says ETS is better. 

## Forecast
The residuals of the model are not normal, so we use bootstrap for prediction intervals:
```{r, echo=FALSE}

fl <- forecast(fitShort, h=12, bootstrap=T)
print(fl)
```
```{r, echo=FALSE, fig.height=5.5, fig.width=10}
plot(fl, xlab="Year", col="red")
```
